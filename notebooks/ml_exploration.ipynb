{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Exploration: Day 2 - Feature Engineering Prep\n",
    "\n",
    "This notebook covers the preparation for machine learning feature engineering:\n",
    "- Feature extraction planning\n",
    "- Train/test split design\n",
    "- Helper similarity functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from scipy.spatial.distance import jaccard\n",
    "\n",
    "# Import our feature engineering module\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from feature_engineering import (\n",
    "    encode_genres_onehot,\n",
    "    vectorize_tags_tfidf,\n",
    "    create_year_features,\n",
    "    FeaturePipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Feature Extraction Planning\n",
    "\n",
    "## Overview\n",
    "\n",
    "For our movie recommendation system, we will extract several types of features:\n",
    "\n",
    "### 1.1 Genre Features (Categorical → Binary)\n",
    "- **Source**: `genres` column (e.g., \"Action|Comedy|Drama\")\n",
    "- **Encoding**: One-hot (multi-label) encoding\n",
    "- **Rationale**: Genres are the primary content descriptor and form the basis of content-based filtering\n",
    "\n",
    "### 1.2 Tag Features (Text → Numeric)\n",
    "- **Source**: User-generated tags\n",
    "- **Encoding**: TF-IDF vectorization\n",
    "- **Rationale**: Tags capture nuanced aspects of movies that genres miss (e.g., \"twist ending\", \"dark humor\")\n",
    "\n",
    "### 1.3 Temporal Features\n",
    "- **Source**: Year extracted from movie title (e.g., \"Toy Story (1995)\")\n",
    "- **Derived Features**:\n",
    "  - Release year (numeric)\n",
    "  - Decade (categorical/numeric)\n",
    "- **Rationale**: User preferences often correlate with movie eras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Feature Extraction Strategy\n",
    "\n",
    "```\n",
    "Raw Data → Preprocessing → Feature Extraction → Feature Matrix\n",
    "\n",
    "movies.csv ─────┬───> Genre One-Hot ────────────┐\n",
    "                │                               │\n",
    "tags.csv ───────┴───> Tag TF-IDF ───────────────┼───> Combined Feature Matrix\n",
    "                │                               │\n",
    "movie titles ───┴───> Year/Decade Extraction ───┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Feature extraction plan demonstration\n",
    "\n",
    "# Sample data for demonstration\n",
    "sample_movies = pd.DataFrame({\n",
    "    'movieId': [1, 2, 3],\n",
    "    'title': ['Toy Story (1995)', 'Jumanji (1995)', 'Heat (1995)'],\n",
    "    'genres': ['Animation|Children|Comedy', 'Adventure|Children|Fantasy', 'Action|Crime|Thriller'],\n",
    "    'tags': ['pixar animated fun', 'jungle adventure board game', 'heist robbery deniro pacino']\n",
    "})\n",
    "\n",
    "print(\"Sample Movies Data:\")\n",
    "print(sample_movies)\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate genre one-hot encoding\n",
    "genre_features, genre_encoder = encode_genres_onehot(sample_movies, 'genres')\n",
    "print(\"Genre One-Hot Encoding:\")\n",
    "print(genre_features)\n",
    "print(f\"\\nGenre classes: {genre_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate year extraction\n",
    "year_features = create_year_features(sample_movies, 'title')\n",
    "print(\"Year Features:\")\n",
    "print(year_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Train/Test Split Design\n",
    "\n",
    "## 2.1 Split Strategy Considerations\n",
    "\n",
    "For recommendation systems, we must carefully consider how to split data:\n",
    "\n",
    "### Random Split\n",
    "- **Use Case**: Standard evaluation of model performance\n",
    "- **Pros**: Simple, unbiased sample\n",
    "- **Cons**: May leak temporal patterns\n",
    "\n",
    "### Temporal Split\n",
    "- **Use Case**: Simulating real-world deployment\n",
    "- **Pros**: More realistic evaluation\n",
    "- **Cons**: May bias toward recent items\n",
    "\n",
    "### User-based Split\n",
    "- **Use Case**: Evaluating cold-start scenarios\n",
    "- **Pros**: Tests generalization to new users\n",
    "- **Cons**: Different user behavior patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Split Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split Configuration\n",
    "\n",
    "SPLIT_CONFIG = {\n",
    "    'test_size': 0.2,           # 20% for testing\n",
    "    'validation_size': 0.1,     # 10% for validation (from training)\n",
    "    'random_state': 42,         # For reproducibility\n",
    "    'stratify': True,           # Stratify by rating distribution\n",
    "}\n",
    "\n",
    "print(\"Split Configuration:\")\n",
    "for key, value in SPLIT_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_test_split(\n",
    "    df: pd.DataFrame,\n",
    "    test_size: float = 0.2,\n",
    "    val_size: float = 0.1,\n",
    "    random_state: int = 42,\n",
    "    stratify_column: str = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create train/validation/test splits for the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe to split\n",
    "    test_size : float\n",
    "        Proportion of data for test set\n",
    "    val_size : float\n",
    "        Proportion of training data for validation set\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    stratify_column : str, optional\n",
    "        Column to use for stratified splitting\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    train_df, val_df, test_df : tuple of DataFrames\n",
    "    \"\"\"\n",
    "    stratify = df[stratify_column] if stratify_column and stratify_column in df.columns else None\n",
    "    \n",
    "    # First split: separate test set\n",
    "    train_val_df, test_df = train_test_split(\n",
    "        df,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=stratify\n",
    "    )\n",
    "    \n",
    "    # Second split: separate validation from training\n",
    "    stratify_val = train_val_df[stratify_column] if stratify_column and stratify_column in df.columns else None\n",
    "    \n",
    "    # Adjust validation size relative to remaining data\n",
    "    val_size_adjusted = val_size / (1 - test_size)\n",
    "    \n",
    "    train_df, val_df = train_test_split(\n",
    "        train_val_df,\n",
    "        test_size=val_size_adjusted,\n",
    "        random_state=random_state,\n",
    "        stratify=stratify_val\n",
    "    )\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temporal_split(\n",
    "    df: pd.DataFrame,\n",
    "    timestamp_column: str,\n",
    "    test_ratio: float = 0.2,\n",
    "    val_ratio: float = 0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Create train/validation/test splits based on timestamp.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with timestamp column\n",
    "    timestamp_column : str\n",
    "        Name of the timestamp column\n",
    "    test_ratio : float\n",
    "        Proportion of most recent data for test set\n",
    "    val_ratio : float\n",
    "        Proportion of data for validation set\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    train_df, val_df, test_df : tuple of DataFrames\n",
    "    \"\"\"\n",
    "    df_sorted = df.sort_values(timestamp_column)\n",
    "    n = len(df_sorted)\n",
    "    \n",
    "    test_start_idx = int(n * (1 - test_ratio))\n",
    "    val_start_idx = int(n * (1 - test_ratio - val_ratio))\n",
    "    \n",
    "    train_df = df_sorted.iloc[:val_start_idx]\n",
    "    val_df = df_sorted.iloc[val_start_idx:test_start_idx]\n",
    "    test_df = df_sorted.iloc[test_start_idx:]\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate splits with sample data\n",
    "\n",
    "sample_ratings = pd.DataFrame({\n",
    "    'userId': [1, 1, 1, 2, 2, 2, 3, 3, 3, 4],\n",
    "    'movieId': [1, 2, 3, 1, 2, 4, 2, 3, 4, 1],\n",
    "    'rating': [4.0, 3.5, 5.0, 4.5, 3.0, 4.0, 5.0, 4.5, 3.5, 4.0],\n",
    "    'timestamp': [1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009]\n",
    "})\n",
    "\n",
    "print(\"Sample Ratings Data:\")\n",
    "print(sample_ratings)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Random split\n",
    "train, val, test = create_train_val_test_split(sample_ratings, test_size=0.2, val_size=0.1)\n",
    "print(f\"\\nRandom Split Sizes: Train={len(train)}, Val={len(val)}, Test={len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Helper Similarity Functions\n",
    "\n",
    "## 3.1 Similarity Metrics Overview\n",
    "\n",
    "For content-based filtering, we need to compute similarity between items:\n",
    "\n",
    "| Metric | Best For | Range |\n",
    "|--------|----------|-------|\n",
    "| Cosine | Sparse vectors (TF-IDF) | [-1, 1] |\n",
    "| Jaccard | Binary features (genres) | [0, 1] |\n",
    "| Euclidean | Dense numeric features | [0, ∞) |\n",
    "| Pearson | Rating patterns | [-1, 1] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(feature_matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute pairwise cosine similarity between all items.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    feature_matrix : np.ndarray\n",
    "        Matrix of shape (n_items, n_features)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    similarity_matrix : np.ndarray\n",
    "        Matrix of shape (n_items, n_items) with pairwise similarities\n",
    "    \"\"\"\n",
    "    return cosine_similarity(feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jaccard_similarity(binary_matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute pairwise Jaccard similarity for binary features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    binary_matrix : np.ndarray\n",
    "        Binary matrix of shape (n_items, n_features)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    similarity_matrix : np.ndarray\n",
    "        Matrix of shape (n_items, n_items) with pairwise Jaccard similarities\n",
    "    \"\"\"\n",
    "    n_items = binary_matrix.shape[0]\n",
    "    similarity_matrix = np.zeros((n_items, n_items))\n",
    "    \n",
    "    for i in range(n_items):\n",
    "        for j in range(i, n_items):\n",
    "            # Jaccard = intersection / union\n",
    "            intersection = np.sum(np.logical_and(binary_matrix[i], binary_matrix[j]))\n",
    "            union = np.sum(np.logical_or(binary_matrix[i], binary_matrix[j]))\n",
    "            \n",
    "            if union == 0:\n",
    "                sim = 0.0\n",
    "            else:\n",
    "                sim = intersection / union\n",
    "            \n",
    "            similarity_matrix[i, j] = sim\n",
    "            similarity_matrix[j, i] = sim\n",
    "    \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_euclidean_similarity(\n",
    "    feature_matrix: np.ndarray,\n",
    "    normalize: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute similarity based on Euclidean distance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    feature_matrix : np.ndarray\n",
    "        Matrix of shape (n_items, n_features)\n",
    "    normalize : bool\n",
    "        If True, normalize distances to [0, 1] range and convert to similarity\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    similarity_matrix : np.ndarray\n",
    "        Matrix of shape (n_items, n_items) with pairwise similarities\n",
    "    \"\"\"\n",
    "    distances = euclidean_distances(feature_matrix)\n",
    "    \n",
    "    if normalize:\n",
    "        # Convert distance to similarity: sim = 1 / (1 + distance)\n",
    "        similarity_matrix = 1 / (1 + distances)\n",
    "    else:\n",
    "        similarity_matrix = -distances  # Negative distance as similarity\n",
    "    \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_similar_items(\n",
    "    similarity_matrix: np.ndarray,\n",
    "    item_idx: int,\n",
    "    top_n: int = 10,\n",
    "    exclude_self: bool = True\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Get the top N most similar items to a given item.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    similarity_matrix : np.ndarray\n",
    "        Precomputed similarity matrix\n",
    "    item_idx : int\n",
    "        Index of the query item\n",
    "    top_n : int\n",
    "        Number of similar items to return\n",
    "    exclude_self : bool\n",
    "        Whether to exclude the item itself from results\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    similar_items : list of tuples\n",
    "        List of (item_idx, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    similarities = similarity_matrix[item_idx]\n",
    "    \n",
    "    if exclude_self:\n",
    "        # Set self-similarity to -inf to exclude\n",
    "        similarities = similarities.copy()\n",
    "        similarities[item_idx] = -np.inf\n",
    "    \n",
    "    # Get indices of top N similar items\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_n]\n",
    "    \n",
    "    return [(idx, similarities[idx]) for idx in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weighted_similarity(\n",
    "    feature_matrices: list,\n",
    "    weights: list,\n",
    "    similarity_fn=cosine_similarity\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute weighted combination of multiple similarity matrices.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    feature_matrices : list of np.ndarray\n",
    "        List of feature matrices (one per feature type)\n",
    "    weights : list of float\n",
    "        Weights for each feature type (should sum to 1)\n",
    "    similarity_fn : callable\n",
    "        Similarity function to use\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    combined_similarity : np.ndarray\n",
    "        Weighted combination of similarity matrices\n",
    "    \"\"\"\n",
    "    if len(feature_matrices) != len(weights):\n",
    "        raise ValueError(\"Number of matrices must match number of weights\")\n",
    "    \n",
    "    # Normalize weights\n",
    "    weights = np.array(weights) / np.sum(weights)\n",
    "    \n",
    "    combined_similarity = None\n",
    "    \n",
    "    for matrix, weight in zip(feature_matrices, weights):\n",
    "        sim_matrix = similarity_fn(matrix)\n",
    "        \n",
    "        if combined_similarity is None:\n",
    "            combined_similarity = weight * sim_matrix\n",
    "        else:\n",
    "            combined_similarity += weight * sim_matrix\n",
    "    \n",
    "    return combined_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate similarity computations\n",
    "\n",
    "# Use genre features from earlier\n",
    "print(\"Genre Features Matrix:\")\n",
    "print(genre_features.values)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Compute cosine similarity\n",
    "cosine_sim = compute_cosine_similarity(genre_features.values)\n",
    "print(\"\\nCosine Similarity Matrix:\")\n",
    "print(np.round(cosine_sim, 3))\n",
    "\n",
    "# Compute Jaccard similarity\n",
    "jaccard_sim = compute_jaccard_similarity(genre_features.values)\n",
    "print(\"\\nJaccard Similarity Matrix:\")\n",
    "print(np.round(jaccard_sim, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most similar movies to Toy Story (index 0)\n",
    "print(\"Movies most similar to 'Toy Story':\")\n",
    "similar_to_toy_story = get_top_similar_items(cosine_sim, item_idx=0, top_n=2)\n",
    "\n",
    "for idx, score in similar_to_toy_story:\n",
    "    print(f\"  {sample_movies.iloc[idx]['title']}: similarity = {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook established the foundation for ML feature engineering:\n",
    "\n",
    "1. **Feature Extraction Planning**: Defined our approach for extracting genre, tag, and temporal features\n",
    "2. **Train/Test Split Design**: Implemented both random and temporal split strategies\n",
    "3. **Similarity Functions**: Created helper functions for computing various similarity metrics\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- [ ] Load actual MovieLens data and apply feature extraction\n",
    "- [ ] Evaluate different similarity metrics on real data\n",
    "- [ ] Build content-based recommendation models\n",
    "- [ ] Implement collaborative filtering approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of helper functions available:\n",
    "\n",
    "print(\"Feature Engineering Functions:\")\n",
    "print(\"  - encode_genres_onehot(): One-hot encode movie genres\")\n",
    "print(\"  - vectorize_tags_tfidf(): TF-IDF vectorize movie tags\")\n",
    "print(\"  - create_year_features(): Extract year and decade from titles\")\n",
    "print(\"  - FeaturePipeline: Complete feature extraction pipeline\")\n",
    "print(\"\")\n",
    "print(\"Split Functions:\")\n",
    "print(\"  - create_train_val_test_split(): Random train/val/test split\")\n",
    "print(\"  - create_temporal_split(): Time-based train/val/test split\")\n",
    "print(\"\")\n",
    "print(\"Similarity Functions:\")\n",
    "print(\"  - compute_cosine_similarity(): Cosine similarity for sparse features\")\n",
    "print(\"  - compute_jaccard_similarity(): Jaccard similarity for binary features\")\n",
    "print(\"  - compute_euclidean_similarity(): Euclidean distance-based similarity\")\n",
    "print(\"  - get_top_similar_items(): Get top-N similar items\")\n",
    "print(\"  - compute_weighted_similarity(): Combine multiple similarity matrices\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
